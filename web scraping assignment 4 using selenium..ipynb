{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad90f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By  \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0fb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d599680",
   "metadata": {},
   "source": [
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details:\n",
    "A) Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date\n",
    "e) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea3d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64197689",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=[]\n",
    "Name=[]\n",
    "Artist=[]\n",
    " \n",
    "date=[]\n",
    "Views=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16a06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[1]'):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"-\")\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[1]/td[1]'):\n",
    "         Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.apend(\"-\")\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[2]/td[3]'):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"-\") \n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[1]/td[3]'):\n",
    "        date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "     date.append(\"-\")\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[2]/tbody/tr[1]/td[3]'):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"-\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc7e66cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>date</th>\n",
       "      <th>Views (in Billions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Baby Shark Dance\"[7] Pinkfong Baby Shark - Ki...</td>\n",
       "      <td>Baby Shark Dance</td>\n",
       "      <td>8.40</td>\n",
       "      <td>14.22</td>\n",
       "      <td>14.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Rank              Name Artist  \\\n",
       "0  \"Baby Shark Dance\"[7] Pinkfong Baby Shark - Ki...  Baby Shark Dance   8.40   \n",
       "\n",
       "    date Views (in Billions)  \n",
       "0  14.22               14.22  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating DataFrame for scraped data\n",
    "Tvideo= pd.DataFrame({})\n",
    "Tvideo['Rank'] = Rank\n",
    "Tvideo['Name'] = Name\n",
    "Tvideo['Artist'] = Artist\n",
    "Tvideo[' date'] = date\n",
    "Tvideo['Views (in Billions)'] = Views\n",
    "\n",
    "# removing stray numbers from Name column\n",
    "Tvideo.Name = Tvideo.Name.apply(lambda x:x[:-4].strip('\"'))\n",
    "Tvideo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7501f745",
   "metadata": {},
   "source": [
    "2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d986dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7049059",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=('https://www.bcci.tv/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7183781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn=driver.find_element(By.XPATH,'/html/body/header/div[3]/div[2]/ul/div[2]/a[2]')\n",
    "driver.get(btn.get_attribute(\"href\"))\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "895d061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Match_Title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec0bf91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Match, Series, Place, Date, Time]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in driver.find_element(By.CLASS_NAME,'matchOrderText ng-binding ng-scope t20-tag'):\n",
    "            Match_Title.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements(By.CLASS_NAME,'match-tournament-name ng-binding'):\n",
    "            Series.append(i.text)\n",
    "                 \n",
    "for i in driver.find_elements(By.XPATH,'ng-binding ng-scope'):\n",
    "            Place.append(i.text)\n",
    "            \n",
    "for i in driver.find_elements(By.XPATH,'match-dates ng-binding'):\n",
    "            Date.append(i.text.replace('\\n',' '))\n",
    "\n",
    "date=[i.split(' ',3)[:3] for i in Date]\n",
    "date=[' '.join(i) for i in date]\n",
    "Time=[i.split(' ',3)[-1] for i in Date]\n",
    "\n",
    "# creating data frame\n",
    "fixture=pd.DataFrame({'Match Title': Match_Title,\n",
    "                          \"Series\": Series,\n",
    "                          \"Place\": Place,\n",
    "                          \"Date\": date,\n",
    "                          \"Time\": Time})\n",
    "fixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details:\n",
    "A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58e94bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"http://statisticstimes.com/ \")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d1b21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on Economy button\n",
    "driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/button\").click()\n",
    "\n",
    "# clicking on India\n",
    "driver.find_element(By.XPATH,\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\").click()\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[1]/ul/li[2]/a\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db1b202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP_billion = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c4654bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/tbody/tr[1]/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "# scraping State\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/thead/tr[1]/th[2] \"):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/thead/tr[1]/th[3] \"):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    \n",
    "# scraping GSDP at current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/thead/tr[1]/th[6]\"):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "    \n",
    "# scraping Share (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/thead/tr[1]/th[4]\"):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "    \n",
    "# scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/div[2]/div[5]/div[1]/div/table/thead/tr[1]/th[5] \"):\n",
    "        GDP_billion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDP_billion.append(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d238ec4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP at current price (19-20)</th>\n",
       "      <th>GSDP at current price (18-19)</th>\n",
       "      <th>Share (18-19)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Rank, State, GSDP at current price (19-20), GSDP at current price (18-19), Share (18-19), GDP($ billion)]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating DataFrame from the scraped data\n",
    "GDP = pd.DataFrame({})\n",
    "GDP['Rank'] = Rank\n",
    "GDP['State'] = State\n",
    "GDP['GSDP at current price (19-20)'] = GSDP1\n",
    "GDP['GSDP at current price (18-19)'] = GSDP2\n",
    "GDP['Share (18-19)'] = Share\n",
    "GDP['GDP($ billion)'] = GDP_billion\n",
    "GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f3090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://github.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fdcdcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting explore button and clicking on it\n",
    "explore = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div/div/div/aside/div[3]/div/a\").click()\n",
    "\n",
    "# selecting trending option\n",
    "trend_url = driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div/div/div/div/main/div/div/div/feed-container/div[4]/turbo-frame/article/div/div/div[1]/header/a')\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2f6b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1757eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each repository\n",
    "repository = driver.find_elements(By.XPATH ,'\"<h5 class=\"text-normal color-fg-muted\">Trending repositories</h5>\"')\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "# scraping Repository title data\n",
    "title = driver.find_elements(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div/div/div/div/main/div/div/div/feed-container/div[4]/turbo-frame/article/div/div/div[2]/div[1]/div/section/div/div[1]/div[1]/a[2]\")\n",
    "for i in title:\n",
    "    repository_title.append(i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e8f59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scraping Repository Description data \n",
    "    try:\n",
    "        desc = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/main/div[3]/div/div[2]/article[6]/p\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    # scraping Contributors Count data\n",
    "    try:\n",
    "        contributor = driver.find_element_by_xpath(By.XPATH,/html/body/div[1]/div[6]/main/div[3]/div/div[2]/article[2]/div[2]/span[2]/a[1]/img\"\")\n",
    "        Contributors.append(contributor.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "    \n",
    "    \n",
    "    # scraping Languages used data\n",
    "    try:\n",
    "        for i in driver.find_elements(By.XPATH,\"/html/body/div[1]/div[6]/main/div[3]/div/div[2]/article[3]/div[2]/span[1]/span[1]\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ca74d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Repository Title, Repository Description, Contributors Count, Language Used]\n",
       "Index: []"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Framing\n",
    "Github = pd.DataFrame({})\n",
    "Github['Repository Title'] = repository_title\n",
    "Github['Repository Description'] = Description\n",
    "Github['Contributors Count'] = Contributors\n",
    "Github['Language Used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930db06e",
   "metadata": {},
   "source": [
    "##5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc271531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.billboard.com/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23c2d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on option button\n",
    "charts=driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[4]/div/div[1]/div[1]/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ca25db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Song_Name = []\n",
    "Artist_Name =[]\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "661348b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting urls for top 100 songs\n",
    "urls = driver.find_element(By.XPATH, \"//li[@class='header__submenu__list__element']//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(4)\n",
    "\n",
    "# scraping data of song names\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div/div[1]/header/div/nav/ul/li[1]/a\"):\n",
    "    Song_Name.append(i.text)\n",
    "    \n",
    "# scraping data of artist names\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[3]/ul/li[4]/ul/li[1]/span\"):\n",
    "    Artist_Name.append(i.text)\n",
    "    \n",
    "# scraping data of last week ranks\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[1]/div[7]\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n",
    "\n",
    "# scraping data of peak ranks\n",
    "for i in driver.find_elements(By.XPATH,\" /html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[1]/div[8]/span\"):\n",
    "    Peak_rank.append(i.text)       \n",
    "    \n",
    "    \n",
    "# scraping data of weeks on board\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[3]/main/div[2]/div[3]/div/div/div/div[2]/div[1]/div[9]/span\"):\n",
    "    Weeks_on_board.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "602c78fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Name, Artist, Last Week Rank, Peak Rank, Weeks on board]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "billiboard = pd.DataFrame({})\n",
    "billiboard['Name'] = Song_Name\n",
    "billiboard['Artist'] = Artist_Name\n",
    "billiboard['Last Week Rank'] = Last_week_rank\n",
    "billiboard['Peak Rank'] = Peak_rank\n",
    "billiboard['Weeks on board'] = Weeks_on_board\n",
    "billiboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ad6923",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre \n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4cba601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\")\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a5ec39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e66b7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping book names data\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/thead/tr/th[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "# scraping author names data\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/thead/tr/th[3] \"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author_name.append('-')\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# scraping data of volumes sold\n",
    "for i in driver.find_elements(By.XPATH,\" /html/body/div/div[2]/div[2]/div/div[2]/div/table/thead/tr/th[4]/div\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "    \n",
    "# scraping data of publisher names\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div/div[2]/div[2]/div/div[2]/div/table/thead/tr/th[5]/div\"):\n",
    "    Publisher.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1df881d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume sold</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title</td>\n",
       "      <td>Author</td>\n",
       "      <td>Volume Sales</td>\n",
       "      <td>Publisher</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Book Name  Author   Volume sold  Publisher\n",
       "0     Title  Author  Volume Sales  Publisher"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "Novels = pd.DataFrame({})\n",
    "Novels['Book Name'] = Book_name\n",
    "Novels['Author'] = Author_name\n",
    "Novels['Volume sold'] = Volumes_sold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details: \n",
    "A) Dataset name \n",
    "B) Data type \n",
    "C) Task \n",
    "D) Attribute type \n",
    "E) No of instances \n",
    "F) No of attribute G) Year \n",
    " Note: - from the home page you have to go to the Show All Dataset page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c16b2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting to the web driver\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "# getting the webpage of mentioned url\n",
    "url = (\" https://archive.ics.uci.edu/\")\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acb2a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/section[1]/div[2]/a \")\n",
    "page_url = viewall_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab962df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching page urls of all datasets\n",
    "view_list = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[1]/div/div/button/span[1]\")\n",
    "list_url = view_list.get_attribute(\"href\")\n",
    "driver.get(list_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf2271b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]\")\n",
    "\n",
    "urls = []\n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efc241e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/div/div/div/a[1]\")\n",
    "\n",
    "urls = []\n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))ear = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c7421194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0c6db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping  Dataset name\n",
    "    try:\n",
    "        dataset_name = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/section[1]/div[2]/div[1]/div/div[2]/h2/a\")\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/section[1]/div[2]/div[3]/div/div[2]/div/div[1]/span\")\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    # scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/section[1]/div[2]/div[1]/div/div[2]/p\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1a40a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(By.XPATH,\"\")\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Instances\n",
    "    try:\n",
    "        instances = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/section[1]/div[2]/div[9]/div/div[2]/div/div[2]/span\")\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # scraping No of Arrtibutes\n",
    "    try:\n",
    "        attribute = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[2]/div[2]/div[1]/div/div[2]/div/div[4]/span\")\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "    time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "698835e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute Type</th>\n",
       "      <th>No of Instance</th>\n",
       "      <th>No of Attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Data Name, Data Type , Task , Attribute Type , No of Instance , No of Attributes , Year ]\n",
       "Index: []"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe for scraped data\n",
    "ML = pd.DataFrame({})\n",
    "ML['Data Name'] = Dataset_name \n",
    "ML['Data Type '] = Data_type\n",
    "ML['Task '] = Task \n",
    "ML['Attribute Type '] = Attribute_type \n",
    "ML['No of Instance '] = No_of_instances\n",
    "ML['No of Attributes '] = No_of_attributes \n",
    "ML['Year '] = Year \n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa479a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
